{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666a507f",
   "metadata": {},
   "source": [
    "Report section\n",
    "====\n",
    "\n",
    "Chosen representation & data preprocessing:\n",
    "----\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "\n",
    "Method extensions and implementation(standard and extended)\n",
    "----\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "    \n",
    "\n",
    "Performance (training and validation) results\n",
    "----\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2557e47b-8c34-466c-b604-6a6af2d83275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8825f43-a7d0-4739-84c8-5dc6d3799363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBayesCLF():\n",
    "    def __init__(self):\n",
    "        self.__data_Class = {}\n",
    "        self.__each_Class_Words = {}\n",
    "        self.__each_Class_Word_Num = {}\n",
    "        self.__p = {}\n",
    "        self.__unique_Words = []\n",
    "        self.__result = []\n",
    "        self.__training_set_size = 0\n",
    "        self.__each_Class_Words_Count = {}\n",
    "        self.stop_Words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for [abstract, label] in zip(X, y):\n",
    "            if label not in self.__data_Class:\n",
    "                self.__data_Class[label] = 1\n",
    "            else:\n",
    "                self.__data_Class[label] += 1\n",
    "            words = self.__preprocess(abstract.split(\" \"))\n",
    "            # words = abstract.split(\" \")\n",
    "            if label not in self.__each_Class_Words:\n",
    "                self.__each_Class_Words[label] = words\n",
    "            else:\n",
    "                self.__each_Class_Words[label] += words\n",
    "        self.__training_set_size = len(X)\n",
    "        self.__calculate_Word_count()\n",
    "        self.__calculate_Unique_Words()\n",
    "        self.__calculate_Prob_Of_Class()\n",
    "        self.__count_Each_Class_Words()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        count = 0\n",
    "        processed_WordList = []\n",
    "        for index, value in X_test.items():\n",
    "            words = self.__preprocess(value.split(\" \"))\n",
    "            # words = X_test[i].split(\" \")\n",
    "\n",
    "            processed_WordList.append(words)\n",
    "            for g in words:\n",
    "                if g not in self.__unique_Words:\n",
    "                    count += 1\n",
    "        result = []\n",
    "        for sample in processed_WordList:\n",
    "            result.append(self.__predict_Item(sample, count))\n",
    "        self.__result = result\n",
    "        return result\n",
    "\n",
    "    def __predict_Item(self, list_Words, count):\n",
    "        result = {label: value for label, value in self.__p.items()}\n",
    "        for word in list_Words:\n",
    "            for label in result.keys():\n",
    "                if word in self.__each_Class_Words[label]:\n",
    "                    result[label] =  math.log((self.__each_Class_Word_Num[label][word]+1) / (self.__each_Class_Words_Count[label] + len(self.__unique_Words) + count), 2)\n",
    "                else:\n",
    "                    result[label] += math.log( 1 / (self.__each_Class_Words_Count[label] + len(self.__unique_Words) + count), 2)\n",
    "        sorted_Result = sorted(result.items(), key = lambda a: a[1], reverse = True)\n",
    "        # print(sorted_Result)\n",
    "        return sorted_Result[0][0]\n",
    "\n",
    "    def __preprocess(self, list_Words):\n",
    "        result = []\n",
    "        for word in list_Words:\n",
    "            # remove stop words\n",
    "            if word in self.stop_Words:\n",
    "                continue\n",
    "            # remove digital words like 1, 2, 3\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            # remove charactor which is meaningless\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            result.append(word)\n",
    "        # print(result)\n",
    "        return result\n",
    "\n",
    "    def results_To_csv(self, fileName):\n",
    "        df = pd.DataFrame([[id, predict] for [id, predict] in zip(range(1, len(\n",
    "            self.__result) + 1), self.__result)], columns=[\"id\", \"class\"]).to_csc(fileName, index=False)\n",
    "\n",
    "    def __calculate_Prob_Of_Class(self):\n",
    "        self.__p = {}\n",
    "        for name in self.__data_Class:\n",
    "            self.__p[name] = math.log(self.__data_Class[name] / self.__training_set_size, 2)\n",
    "        # print(self.__p)\n",
    "\n",
    "    def __calculate_Word_count(self):\n",
    "        for y in self.__data_Class.keys():\n",
    "            if y not in self.__each_Class_Word_Num:\n",
    "                self.__each_Class_Word_Num[y] = {}\n",
    "            for word in self.__each_Class_Words[y]:\n",
    "                if word not in self.__each_Class_Word_Num[y]:\n",
    "                    self.__each_Class_Word_Num[y][word] = 1\n",
    "                else:\n",
    "                    self.__each_Class_Word_Num[y][word] += 1\n",
    "        # print(self.__each_Class_Word_Num)\n",
    "\n",
    "    def __calculate_Unique_Words(self):\n",
    "        words = []\n",
    "        for y in self.__each_Class_Word_Num.keys():\n",
    "            words += self.__each_Class_Word_Num[y].keys()\n",
    "        self.__unique_Words = set(words)\n",
    "        # print(self.__unique_Words)\n",
    "    def __count_Each_Class_Words(self):\n",
    "        for label in self.__each_Class_Words:\n",
    "            self.__each_Class_Words_Count[label] = len(self.__each_Class_Words[label])\n",
    "        # print(self.__each_Class_Words_Count)\n",
    "\n",
    "    def __str__(self):\n",
    "        msg = \"\"\n",
    "        msg += f\"Class in the training set: {self.__data_Class}\\n\"\n",
    "        msg += f\"Words in each class: {self.__each_Class_Words}\\n\"\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f459b78c-d6f0-4813-9378-d6b798079b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFolds:\n",
    "    def __init__(self, n_splits, shuffle = True, seed = 4321):\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def split(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        if self.shuffle:\n",
    "            rstate = np.random.RandomState(self.seed)\n",
    "            rstate.shuffle(indices)\n",
    "        for test_mask in self._iter_test_masks(n_samples, indices):\n",
    "            train_index = indices[np.logical_not(test_mask)]\n",
    "            test_index = indices[test_mask]\n",
    "            train_set = X.filter(train_index, axis = 0)\n",
    "            test_set = X.filter(test_index, axis = 0)\n",
    "            yield train_set, test_set\n",
    "        \n",
    "    def _iter_test_masks(self, n_samples, indices):\n",
    "        fold_sizes = (n_samples // self.n_splits) * np.ones(self.n_splits, dtype = np.int64)\n",
    "        fold_sizes[:n_samples % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            test_indices = indices[start:stop]\n",
    "            test_mask = np.zeros(n_samples, dtype = bool)\n",
    "            test_mask[test_indices] = True\n",
    "            yield test_mask\n",
    "            current = stop\n",
    "def calScore(pred_List, real_List):\n",
    "    count = 0\n",
    "    for pred, real in zip(pred_List, real_List):\n",
    "        if pred == real:\n",
    "            count += 1\n",
    "    return count / len(real_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036fa6a-8e14-48a8-9317-ccc4420bcf44",
   "metadata": {},
   "source": [
    "Cross validation on the training set and find the scores of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef3e760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.645, 0.65875, 0.66, 0.625, 0.65]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv(os.path.join(\"data\", \"trg.csv\"))\n",
    "test_Set = pd.read_csv(os.path.join(\"data\", \"tst.csv\"))\n",
    "\n",
    "scores = []\n",
    "kfold = KFolds(5)\n",
    "for train,valid in kfold.split(training_set):\n",
    "    myCLF = MyBayesCLF()\n",
    "    myCLF.fit(X = train[\"abstract\"], y = train[\"class\"])\n",
    "    \n",
    "    result = myCLF.predict(valid[\"abstract\"])\n",
    "    score = calScore(result, valid[\"class\"])\n",
    "    scores.append(score)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dcf337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 391, 'E': 452, 'V': 55, 'A': 102}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "count = {}\n",
    "for item in result:\n",
    "    if item in count:\n",
    "        count[item]+= 1\n",
    "    else:\n",
    "        count[item] = 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1e76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Set.insert(1, \"class\", result)\n",
    "test_Set.drop([\"abstract\"], axis=1).to_csv('out.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
