{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: -986.5248509778967, A: -1027.7391021585486, E: -1055.1280287755546, V: -1043.2897454574088\n",
      "B: -1535.8885245237864, A: -1550.4675342986118, E: -1507.6492147588247, V: -1560.5800449099452\n",
      "B: -2251.54415312804, A: -2356.7510329817674, E: -2075.1705755196367, V: -2284.8808584276926\n",
      "B: -1171.3034166423618, A: -1207.8970096798869, E: -1084.086689381797, V: -1193.9000613230903\n",
      "B: -1553.6861282104414, A: -1597.862728384032, E: -1430.1047153039206, V: -1578.0240259228215\n",
      "B: -2410.886019981124, A: -2565.803853768431, E: -2296.5527641128883, V: -2476.3144188198785\n",
      "B: -1184.022836374005, A: -1202.2840336414467, E: -1135.361029034213, V: -1190.9818591998367\n",
      "B: -1056.0303283203382, A: -1246.0106576422745, E: -1225.475868419761, V: -1306.2483929334774\n",
      "B: -1041.7960161879755, A: -1075.5782639239922, E: -982.1419731248643, V: -1075.76456320277\n",
      "B: -851.5290012203732, A: -959.9139433597845, E: -896.7931298134691, V: -918.7437750757255\n",
      "['B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "def preprocess(list_Words):\n",
    "        result = []\n",
    "        stop_Words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "        for word in list_Words:\n",
    "            # remove stop words\n",
    "            if word in stop_Words:\n",
    "                continue\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            result.append(word)\n",
    "            \n",
    "        # print(result)\n",
    "        return result\n",
    "\n",
    "training_set = pd.read_csv(\"./data/trg.csv\")\n",
    "train_class = training_set[\"class\"]\n",
    "train_abstract = training_set[\"abstract\"]\n",
    "\n",
    "count_Train_A = 0\n",
    "count_Train_B = 0\n",
    "count_Train_E = 0\n",
    "count_Train_V = 0\n",
    "\n",
    "length_Of_Train = len(train_class)\n",
    "A = []\n",
    "B = []\n",
    "E = []\n",
    "V = []\n",
    "\n",
    "for i in range(0,len(train_class)):\n",
    "    # word = train_abstract[i].split(\" \")\n",
    "    word = preprocess(train_abstract[i].split(\" \"))\n",
    "    if train_class[i] == \"A\":\n",
    "        count_Train_A += 1\n",
    "        A = A + word   \n",
    "    elif train_class[i] == \"B\":\n",
    "        count_Train_B += 1\n",
    "        B = B + word\n",
    "    elif train_class[i] == \"E\":\n",
    "        count_Train_E += 1\n",
    "        E = E + word\n",
    "    else:\n",
    "        count_Train_V += 1\n",
    "        V = V + word\n",
    "\n",
    "dict_a = {}\n",
    "dict_b = {}\n",
    "dict_e = {}\n",
    "dict_v = {}\n",
    "\n",
    "for w in A:\n",
    "    if w not in dict_a:\n",
    "        dict_a[w] = 1\n",
    "    else:\n",
    "        dict_a[w] += 1\n",
    "\n",
    "for w in B:\n",
    "    if w not in dict_b:\n",
    "        dict_b[w] = 1\n",
    "    else:\n",
    "        dict_b[w] += 1\n",
    "        \n",
    "for w in E:\n",
    "    if w not in dict_e:\n",
    "        dict_e[w] = 1\n",
    "    else:\n",
    "        dict_e[w] += 1\n",
    "        \n",
    "for w in V:\n",
    "    if w not in dict_v:\n",
    "        dict_v[w] = 1\n",
    "    else:\n",
    "        dict_v[w] += 1\n",
    "\n",
    "unique_words = []\n",
    "for i in dict_a:\n",
    "    if i not in unique_words:\n",
    "        unique_words += [i]\n",
    "for i in dict_b:\n",
    "    if i not in unique_words:\n",
    "        unique_words += [i]\n",
    "for i in dict_e:\n",
    "    if i not in unique_words:\n",
    "        unique_words += [i]\n",
    "for i in dict_v:\n",
    "    if i not in unique_words:\n",
    "        unique_words += [i]\n",
    "count_a_words = len(A)\n",
    "count_b_words = len(B)\n",
    "count_e_words = len(E)\n",
    "count_v_words = len(V)\n",
    "\n",
    "def classify(abstracts, dict_a, dict_b, dict_e, dict_v, count_Train_A, count_Train_B, count_Train_E, count_Train_V, unique_words, length_Of_Train, count_a_words, count_b_words, count_e_words, count_v_words):\n",
    "    uni = len(unique_words)\n",
    "    na = len(dict_a)\n",
    "    nb = len(dict_b)\n",
    "    ne = len(dict_e)\n",
    "    nv = len(dict_v)\n",
    "    a_list = []\n",
    "    num = len(abstracts)\n",
    "    count = 0\n",
    "    for i in range(0, num):\n",
    "        # words = abstracts[i].split(\" \") \n",
    "        words = preprocess(abstracts[i].split(\" \") )\n",
    "\n",
    "        for g in words:\n",
    "            if g not in unique_words:\n",
    "                count += 1\n",
    "                \n",
    "    for i in range(0, num):\n",
    "        # words = abstracts[i].split(\" \") \n",
    "        words = preprocess(abstracts[i].split(\" \") )\n",
    "\n",
    "        pa = math.log(count_Train_A / length_Of_Train, 2)\n",
    "#         print(pa)\n",
    "        pb = math.log(count_Train_B / length_Of_Train, 2)\n",
    "#         print(pb)\n",
    "        pe = math.log(count_Train_E / length_Of_Train, 2)\n",
    "        pv = math.log(count_Train_V / length_Of_Train, 2)\n",
    "        for word in words:\n",
    "            if word in dict_a:\n",
    "               \n",
    "                pa = pa + math.log((dict_a[word]+1) / (count_a_words + uni + count), 2)\n",
    "            else:\n",
    "                pa = pa + math.log(1 / (count_a_words + uni + count), 2)\n",
    "            \n",
    "            if word in dict_b:\n",
    "               \n",
    "                pb = pb + math.log((dict_b[word]+1) / (count_b_words + uni + count), 2)\n",
    "            else:\n",
    "                pb = pb + math.log(1 / (count_b_words + uni + count), 2)\n",
    "            \n",
    "            if word in dict_e:\n",
    "                pe = pe + math.log((dict_e[word]+1) / (count_e_words + uni + count), 2)\n",
    "            else:\n",
    "                pe = pe + math.log(1 / (count_e_words + uni + count), 2)\n",
    "                \n",
    "            if word in dict_v:\n",
    "                pv = pv + math.log((dict_v[word]+1) / (count_v_words + uni + count), 2)\n",
    "            else:\n",
    "                pv = pv + math.log(1 / (count_v_words + uni + count), 2)       \n",
    "        print(f\"B: {pb}, A: {pa}, E: {pe}, V: {pv}\")\n",
    "        max_num = max(pa, pb, pe, pv)\n",
    "        if max_num == pa:\n",
    "            a_list = a_list + [\"A\"]\n",
    "        elif max_num == pb:\n",
    "            a_list = a_list + [\"B\"]\n",
    "        elif max_num == pe:\n",
    "            a_list = a_list + [\"E\"]\n",
    "        else:\n",
    "            a_list = a_list + [\"V\"]\n",
    "    print(a_list)\n",
    "#     print(a_list)\n",
    "    return a_list \n",
    "\n",
    "\n",
    "test_set = pd.read_csv(\"./data/tst.csv\")\n",
    "test_set[\"abstract\"]\n",
    "                      \n",
    "test_set_class_predictions = classify(test_set[\"abstract\"][:10], dict_a, dict_b, dict_e, dict_v, count_Train_A, count_Train_B, count_Train_E, count_Train_V, unique_words, length_Of_Train, count_a_words , count_b_words, count_e_words, count_v_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_class_predictions\n",
    "countEach = {}\n",
    "for pre in test_set_class_predictions:\n",
    "    if pre not in countEach:\n",
    "        countEach[pre] = 1\n",
    "    else:\n",
    "        countEach[pre] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.insert(1, \"class\", test_set_class_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.drop([\"abstract\"], axis=1).to_csv('out.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 386, 'E': 565, 'A': 30, 'V': 19}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "predict = pd.read_csv(\"out.csv\")\n",
    "result = predict[\"class\"]\n",
    "count = {}\n",
    "for item in result:\n",
    "    if item in count:\n",
    "        count[item]+= 1\n",
    "    else:\n",
    "        count[item] = 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 3)\n",
      "(800, 3)\n",
      "(3200, 3)\n",
      "(800, 3)\n",
      "(3200, 3)\n",
      "(800, 3)\n",
      "(3200, 3)\n",
      "(800, 3)\n",
      "(3200, 3)\n",
      "(800, 3)\n"
     ]
    }
   ],
   "source": [
    "def score(pred_List, real_List):\n",
    "    count = 0\n",
    "    for pred, real in zip(pred_List, real_List):\n",
    "        if pred == real:\n",
    "            count += 1\n",
    "    return count / len(real_List)\n",
    "\n",
    "training_set.drop([1, 2,3])\n",
    "kfold = KFolds(5)\n",
    "for train,valid in kfold.split(training_set):\n",
    "    print(train.shape)\n",
    "    print(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24875]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "training_set = pd.read_csv(os.path.join(\"data\", \"trg.csv\"))\n",
    "test_set = pd.read_csv(os.path.join(\"data\", \"tst.csv\"))\n",
    "\n",
    "class KFolds:\n",
    "    def __init__(self, n_splits, shuffle = True, seed = 4321):\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.n_splits = n_splits\n",
    "    # iterable split function, call it in a for loop and it will iter each train and validation\n",
    "    def split(self, X):\n",
    "        num_Of_Samples = X.shape[0]\n",
    "        indices = np.arange(num_Of_Samples)\n",
    "        if self.shuffle:\n",
    "            random_State = np.random.RandomState(self.seed)\n",
    "            random_State.shuffle(indices)\n",
    "        for test_mask in self._iter_test_masks(num_Of_Samples, indices):\n",
    "            train_index = indices[np.logical_not(test_mask)]\n",
    "            test_index = indices[test_mask]\n",
    "            train_set = X.filter(train_index, axis = 0)\n",
    "            test_set = X.filter(test_index, axis = 0)\n",
    "            yield train_set, test_set\n",
    "        \n",
    "    def _iter_test_masks(self, num_Of_Samples, indices):\n",
    "        fold_sizes = (num_Of_Samples // self.n_splits) * np.ones(self.n_splits, dtype = np.int64)\n",
    "        fold_sizes[:num_Of_Samples % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            test_indices = indices[start:stop]\n",
    "            test_mask = np.zeros(num_Of_Samples, dtype = bool)\n",
    "            test_mask[test_indices] = True\n",
    "            yield test_mask\n",
    "            current = stop\n",
    "def calScore(pred_List, real_List):\n",
    "    count = 0\n",
    "    for pred, real in zip(pred_List, real_List):\n",
    "        if pred == real:\n",
    "            count += 1\n",
    "    return count / len(real_List)\n",
    "\n",
    "class StandardBayesCLF:\n",
    "    def __init__(self):\n",
    "        self.__data_Class = {}\n",
    "        self.__each_Class_Words = {}\n",
    "        self.__each_Class_Word_Num = {}\n",
    "        self.__p = {}\n",
    "        self.__unique_Words = []\n",
    "        self.__result = []\n",
    "        self.__training_set_size = 0\n",
    "        self.__each_Class_Words_Count = {}\n",
    "        self.stop_Words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit the training data, X ,y\n",
    "        \"\"\"\n",
    "        for [abstract, label] in zip(X, y):\n",
    "            if label not in self.__data_Class:\n",
    "                self.__data_Class[label] = 1\n",
    "            else:\n",
    "                self.__data_Class[label] += 1\n",
    "            words = abstract.split(\" \")\n",
    "            if label not in self.__each_Class_Words:\n",
    "                self.__each_Class_Words[label] = words\n",
    "            else:\n",
    "                self.__each_Class_Words[label] += words\n",
    "        self.__training_set_size = len(X)\n",
    "        self.__calculate_Word_count()\n",
    "        self.__calculate_Prob_Of_Class()\n",
    "        self.__count_Each_Class_Words()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        count = 0\n",
    "        processed_WordList = []\n",
    "        for index, value in X_test.items():\n",
    "            words = value.split(\" \")\n",
    "\n",
    "            processed_WordList.append(words)\n",
    "            for g in words:\n",
    "                if g not in self.__unique_Words:\n",
    "                    count += 1\n",
    "        result = []\n",
    "        for sample in processed_WordList:\n",
    "            result.append(self.__predict_Item(sample, count))\n",
    "        self.__result = result\n",
    "        return result\n",
    "\n",
    "    def __predict_Item(self, list_Words, count):\n",
    "        prob_Dict = {label: value for label, value in self.__p.items()}\n",
    "#         print(prob_Dict)\n",
    "#         print(list_Words)\n",
    "        for word in list_Words:\n",
    "            for label in prob_Dict.keys():\n",
    "                if word in self.__each_Class_Words[label]:\n",
    "                    prob_Dict[label] *=  (self.__each_Class_Word_Num[label][word]+1) / (self.__each_Class_Words_Count[label] )\n",
    "#         print(prob_Dict)\n",
    "        sorted_prob_Dict = sorted(prob_Dict.items(), key = lambda a: a[1], reverse = True)\n",
    "#         print(sorted_prob_Dict)\n",
    "        return sorted_prob_Dict[0][0]\n",
    "\n",
    "\n",
    "    def results_To_csv(self, fileName):\n",
    "        df = pd.DataFrame([[id, predict] for [id, predict] in zip(range(1, len(\n",
    "            self.__result) + 1), self.__result)], columns=[\"id\", \"class\"]).to_csv(fileName, index=False)\n",
    "\n",
    "    def __calculate_Prob_Of_Class(self):\n",
    "        self.__p = {}\n",
    "        for name in self.__data_Class:\n",
    "            self.__p[name] = self.__data_Class[name] / self.__training_set_size\n",
    "#         print(self.__p)\n",
    "\n",
    "    def __calculate_Word_count(self):\n",
    "        \"\"\"\n",
    "        calculate each word appear time in each class\n",
    "        \"\"\"\n",
    "        for y in self.__data_Class.keys():\n",
    "            if y not in self.__each_Class_Word_Num:\n",
    "                self.__each_Class_Word_Num[y] = {}\n",
    "            for word in self.__each_Class_Words[y]:\n",
    "                if word not in self.__each_Class_Word_Num[y]:\n",
    "                    self.__each_Class_Word_Num[y][word] = 1\n",
    "                else:\n",
    "                    self.__each_Class_Word_Num[y][word] += 1\n",
    "        # print(self.__each_Class_Word_Num)\n",
    "\n",
    "    def __count_Each_Class_Words(self):\n",
    "        for label in self.__each_Class_Words:\n",
    "            self.__each_Class_Words_Count[label] = len(self.__each_Class_Words[label])\n",
    "        # print(self.__each_Class_Words_Count)\n",
    "\n",
    "    def __str__(self):\n",
    "        msg = \"\"\n",
    "        msg += f\"Class in the training set: {self.__data_Class}\\n\"\n",
    "        msg += f\"Words in each class: {self.__each_Class_Words}\\n\"\n",
    "        return msg\n",
    "scores = []\n",
    "kfold = KFolds(5)\n",
    "for train,valid in kfold.split(training_set):\n",
    "    myCLF = StandardBayesCLF()\n",
    "    myCLF.fit(X = train[\"abstract\"], y = train[\"class\"])\n",
    "    \n",
    "    result = myCLF.predict(valid[\"abstract\"])\n",
    "    score = calScore(result, valid[\"class\"])\n",
    "    scores.append(score)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
