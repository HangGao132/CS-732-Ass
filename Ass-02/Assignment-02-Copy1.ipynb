{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d54c0e-9c4e-47e9-8ed0-6805c36e35e6",
   "metadata": {},
   "source": [
    "# Assignment Report\n",
    "\n",
    "## Task 1: load data and fix missing data\n",
    "Fisrt, load the data. Notice that there is no header in the csv file, so we set header = None while loading. Then, I found that each feature has only 1 missing data, but 1000 data. So either we remove the row, or we can replace the missing data with mean value. I used mean to replace the missing value, because mean is most effective. You can not add constant value, because the range of each feature varies, so adding mean is neither too high or too low, and will not add noise to the dataset. So useing mean can preserve the row of data, and this missing data will be considered important by the tree.\n",
    "## Task 2: 10 most important features\n",
    "To decide the most important features, I use PCA(principal component analysis) to perform. The method PCA is measure the variation in each feature, and find the features with the highest variance. The more variation in the feature, the data will be more widely distributed, which is better for a Decision Tree classifier to use as decision point. To test the theory, I printed out the 20 most important features and their variance, we can notice that the top 10 features have variances over 18, and from the 11th feature, the cariances will be around 1. So we can tell that the first 10 features has more variance, and are better for Decision Tree to learn.\n",
    "\n",
    "## Task 3\n",
    "### 3.1 Repetitions\n",
    "In order to get enough repeatation, I used the method in assignment 1, which is repeating 100 times and in each loop, perfrom a different train and test split, and perfrom the models, get the results, and use autorank to find if there is difference.  \n",
    "### 3.2 Pruning\n",
    "As for pruning, some parameters for Decision Tree are max depth, min sample split, min samples leaf. If you give these parameters a value, the branch will stop growing when it meets the requirments. So what I did was, using each of the parameters as independent variable, and find which settings has the best validation score. Then use these parameters for pruned Decision Tree. To perform pruning on the validation set, I use <code>\n",
    "model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "</code>. By using \"cross_val_score\", training set will be devided into another train and validition set 10 times, and calculate the average score for this model. \n",
    "### 3.3 Result and findings\n",
    "After I finished the training, here are the findings from autorank:  \n",
    "We reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Random Forest (M=0.709+-0.005, SD=0.022, MR=1.035), Pruned Decision Tree (M=0.640+-0.007, SD=0.027, MR=2.540), Unpruned Decision Tree (M=0.641+-0.007, SD=0.027, MR=2.565), and Decision Stamp (M=0.598+-0.006, SD=0.022, MR=3.860). Therefore, we assume that there is a statistically significant difference between the median values of the populations.  \n",
    "So we can say that the best is Random Forest, then Pruned Decision Tree, Unpruned Decision Tree, and the worest is Decision stamp. Decision stamp is the worst is because it has only 1 layer, which is underfitting. We use 10 features and 1000 dataset, apparently one layer is not enough. Pruned is better than unpruned, because it prevent overfitting. And the Random Forest is the best because it has many different trees in the model, which makes it a strong classification model, it can be more accurate.\n",
    "\n",
    "## Task 4\n",
    "### 4.1 Adding 20% noise\n",
    "The code is given by Luke:\n",
    "<code>\n",
    "noise = np.random.normal(0, 0.2, np.shape(X_Top10_Features))\n",
    "X_noise_add = X_Top10_Features + np.multiply(noise, np.average(X_Top10_Features, axis=0))\n",
    "</code>\n",
    "What the code does is first generate a noise weight matrix with the same shape of the set of X, and each noise is generated using normal weight distribution with a standard diviation of 0.2. Then we multiply the noise weights and the mean of the feature, using numpy.multiply() function to get the noise. Then we add the noise matrix with the original feature matrix to get the features with noise.  \n",
    "\n",
    "\n",
    "### 4.2 Result and findings\n",
    "With the same training steps as step3, I trained the models. Here are the findings: \n",
    "The result of the autorank is:  \n",
    "\"We reject the null hypothesis (p=0.000) of the repeated measures ANOVA that there is a difference between the mean values of the populations Random Forest (M=0.691+-0.004, SD=0.025), Pruned Decision Tree (M=0.631+-0.004, SD=0.026), Unpruned Decision Tree (M=0.622+-0.004, SD=0.026), and Decision Stamp (M=0.583+-0.004, SD=0.021). Therefore, we assume that there is a statistically significant difference between the mean values of the populations.\"  \n",
    "Then we can tell that the best is still Random Forest, and its performance slightly decrease 0.02, because Random Forest model has many trees, the noise can be resist by the combination of different trees, in other words, the noise pattern can also be learned by Random Forest and still it proformed well. The pruned and unpruned Decision Tree models has performed significantly worse. Because they have only one tree and can be greatly influenced by noise, especially unpruned one because it is overfitting to the noise. The Decision Stamp has the same performance because it just roughly classify the data into two classes, and will ignore the noise in this way, just focus on the most significant feature.\n",
    "\n",
    "## Task 5\n",
    "### 5.1 Multiply 20% noise\n",
    "The code for multiplicative noise is  \n",
    "<code>\n",
    "noise = np.random.normal(1, 0.2, np.shape(X_Top10_Features))\n",
    "X_noise_multiply = np.multiply(X_Top10_Features, noise)\n",
    "</code>\n",
    "First, it will generate the noise weight matrix with the same shape of feature matrix. Then we just get the matrix product of noise weight and original feature using numpy.multiply() function.  \n",
    "Then, I performed the training steps same as step 3.\n",
    "### 5.2 Result and findings\n",
    "Here is the result of the autorank:  \n",
    "\"We reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Random Forest (M=0.638+-0.006, SD=0.023, MR=1.055), Pruned Decision Tree (M=0.585+-0.007, SD=0.029, MR=2.645), Unpruned Decision Tree (M=0.582+-0.006, SD=0.025, MR=2.705), and Decision Stamp (M=0.554+-0.005, SD=0.022, MR=3.595). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\"  \n",
    "So the best classifier is still Random Forest, and the worst is Decision Stamp. Random Forest, pruned and unpruned Random Forest perform significantly worse than using the original top 10 features. Because multiply the noise can greatly affect the data set. Fisrt, there is 50% chance that the signs of the data will be fliped. And it will add more outliers. So even Random Forest will be affected. Only dicision stamp drop from 0.58 to 0.55 both of which is close to random guess.\n",
    "\n",
    "## Task 6\n",
    "### 6.1 Flip labels with 5% posibility\n",
    "First transformt the pandas dataframe of y to numpy array called y_np. Loop through the labels, each loop we generate a random value, if the value is less than 0.05, then flip the label.\n",
    "    \n",
    "### 6.2 Result and findings\n",
    "Then train and test the models. And use autorank to analysis the scores. Here are the result:  \n",
    "\"We reject the null hypothesis (p=0.000) of the repeated measures ANOVA that there is a difference between the mean values of the populations Random Forest (M=0.691+-0.004, SD=0.025), Pruned Decision Tree (M=0.631+-0.004, SD=0.026), Unpruned Decision Tree (M=0.622+-0.004, SD=0.026), and Decision Stamp (M=0.583+-0.004, SD=0.021). Therefore, we assume that there is a statistically significant difference between the mean values of the populations.\"    \n",
    "  \n",
    "Then, we can tell that the best is Random Forest. Random Forest is very robost for the label noise. But for pruned and unpruned Decision trees, the performance decrease siginificantly. For decision stamp, it does not change. To explain the Random Forest, there is a concept called biasâ€“variance decomposition. It means that for each different decision tree has 60% accuracy, when given a new instance, about 60% of trees will get the right answer. Then the Random Forest will perform a majority vote, so the performance will still be good. For dicision stamp, it is still roughly classificition, and the label noise will not affect much. For Decision Trees, the model can be affected greatly, so the performance will drop.\n",
    "\n",
    "## Task 7 Add 20% seperately\n",
    "### 7.1 Add to training set\n",
    "### 7.1 Add to test set\n",
    "\n",
    "## Task Bonus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4135caf-13ea-4746-9644-20117466f280",
   "metadata": {},
   "source": [
    "Task 1:\n",
    "----\n",
    "Load the data and replace missingd data with the mean value of all other data in this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628c2cef-9efe-4fa2-b126-cb88590e7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from autorank import autorank, create_report, plot_stats\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "RANDOM_STATE = 1234\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c1846a-cefc-4c46-8de9-dbc232c9657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is read\n",
      "           0         1         2         3         4         5         6   \\\n",
      "0         NaN -0.176595 -4.126644 -2.390884 -1.366659 -0.460177  1.521421   \n",
      "1    0.267386       NaN -1.820798  1.540020  1.656770  0.447912  6.484100   \n",
      "2    2.946515 -1.594067       NaN  0.403844 -1.733342  0.274193  2.960081   \n",
      "3   -4.581082 -0.001274  1.197022       NaN -0.416290 -1.941236  0.290991   \n",
      "4   -2.123360 -0.266587  1.781999 -0.039342       NaN  0.895856  3.756880   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "995  0.318539  0.539550 -0.144831  1.402720  2.018928 -0.033435 -0.763604   \n",
      "996 -5.843779  0.341371 -0.612153 -0.209380 -1.398469  0.550250 -1.623379   \n",
      "997  3.406411  0.321762  0.286231  2.116516 -1.548059 -1.626494  3.787104   \n",
      "998 -2.167186  0.627939 -0.993588  0.902711 -1.024744 -0.416702  3.966772   \n",
      "999 -3.621659 -1.628561 -1.991653  3.852027  1.636139 -0.283941  0.050966   \n",
      "\n",
      "           7         8         9   ...        90        91        92  \\\n",
      "0    5.328470 -2.919639 -6.216557  ... -1.919015 -1.603442 -0.867395   \n",
      "1    2.356129 -1.129611  3.195601  ...  1.394421 -1.872976 -0.382098   \n",
      "2    0.326221  0.393324  0.229915  ... -0.290845 -0.076589 -4.660688   \n",
      "3    0.507716 -0.397205  4.399152  ...  1.046033 -4.498888  7.150844   \n",
      "4   -4.583167  1.652762  7.673453  ...  1.371734  4.458045  3.576609   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995 -4.475018  1.098775  2.187650  ...  1.163138  1.427695  0.753621   \n",
      "996 -0.924526 -2.274803 -3.090194  ...  1.998539 -0.695267  1.238735   \n",
      "997  2.476920 -2.113878 -7.427621  ... -2.762413 -2.860776 -6.281361   \n",
      "998  6.068817 -2.394340 -1.352014  ... -0.981470 -2.427518 -0.065137   \n",
      "999 -1.044173 -2.044518 -4.157472  ...  0.518181 -0.485885  0.153331   \n",
      "\n",
      "           93        94        95        96        97        98        99  \n",
      "0   -1.825951 -2.390884  4.107535  3.817897 -0.823303 -5.271267  2.353398  \n",
      "1   -1.980864  1.540020 -1.267888 -1.843256  1.105676 -0.344550 -6.653773  \n",
      "2   -1.652196  0.403844 -4.317565 -2.506476  3.422634 -0.272146 -3.345401  \n",
      "3    0.722794 -2.944502 -0.081950  0.186089 -7.066950  2.488842 -2.770303  \n",
      "4   -1.538929 -0.039342 -2.613114 -3.117484 -2.125028  0.143158 -8.249535  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "995  0.467884  1.402720       NaN -2.008609  0.279494  4.702728 -2.272688  \n",
      "996  1.192938 -0.209380 -0.972033       NaN -2.624541  0.013214  1.422758  \n",
      "997 -3.061634  2.116516  6.246774 -0.847790       NaN -8.109827  2.108502  \n",
      "998 -1.653230  0.902711  1.644251 -0.421722 -3.231510       NaN -1.626739  \n",
      "999 -0.867391  3.852027  3.559970  0.754794  0.072521 -2.605920       NaN  \n",
      "\n",
      "[1000 rows x 100 columns]\n",
      "y read\n",
      "       0\n",
      "0    1.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    1.0\n",
      "..   ...\n",
      "995  1.0\n",
      "996  0.0\n",
      "997  0.0\n",
      "998  1.0\n",
      "999  1.0\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(os.path.join(\"data\", \"data_A2.csv\"), header = None)\n",
    "y = pd.read_csv(os.path.join(\"data\", \"labels_A2.csv\"), header = None)\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "print(\"X is read\")\n",
    "print(X)\n",
    "print(\"y read\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4d866-0618-4f88-b481-8b152caee7cc",
   "metadata": {},
   "source": [
    "Missing data\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d70143-d10d-49c6-b520-0b7edc237cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after fixed missing data\n",
      "[[-0.03640934 -0.17659526 -4.12664374 ... -0.82330291 -5.27126666\n",
      "   2.35339793]\n",
      " [ 0.26738568  0.01051725 -1.8207978  ...  1.10567555 -0.34454961\n",
      "  -6.6537733 ]\n",
      " [ 2.94651515 -1.5940672   0.53939977 ...  3.42263394 -0.27214566\n",
      "  -3.34540104]\n",
      " ...\n",
      " [ 3.40641099  0.32176174  0.28623093 ...  0.1004745  -8.10982707\n",
      "   2.10850165]\n",
      " [-2.16718563  0.62793907 -0.99358794 ... -3.23150977 -0.15775602\n",
      "  -1.62673851]\n",
      " [-3.62165864 -1.62856111 -1.99165251 ...  0.07252107 -2.60592028\n",
      "  -0.26877021]]\n"
     ]
    }
   ],
   "source": [
    "# step 1: use SimpleImputer module to replace all the np.nan value with the mean value of the feature\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X)\n",
    "X_Fixed_Miss = imp.transform(X)\n",
    "print(\"X after fixed missing data\")\n",
    "print(X_Fixed_Miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b7e00-affb-4805-8d8c-b0f3bc8e223c",
   "metadata": {},
   "source": [
    "Task 2:\n",
    "----\n",
    "Find the 10 most important features, using the Random Forest package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f5dd61-f753-476e-8804-2dceb7af28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model has method to select best features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02631fa-180b-4e58-bf69-280a89886c93",
   "metadata": {},
   "source": [
    "Task 3\n",
    "----\n",
    "Perform Random Forest, Decision tree, and pruned tree, and decision stamp, on the dataset from task2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b9f0ed-9330-4bbe-a64d-eba3e3465e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_Top10_Features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-82a98d9c156f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# As for pruning, in order to find out the best hyper parameters for pruning, we can use cross validation to test the score for each parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Top10_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRANDOM_STATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# tuning max_depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_Top10_Features' is not defined"
     ]
    }
   ],
   "source": [
    "# As for pruning, in order to find out the best hyper parameters for pruning, we can use cross validation to test the score for each parameter\n",
    "random_state = 2**16\n",
    "np.random.seed(random_state)\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "for idx_train, idx_test in kfold.split(X, y):\n",
    "    y_train = y[idx_train]\n",
    "    y_test = y[idx_test]\n",
    "    print('[Train] Positive : {}, Negative: {}, Ratio: {:.2f}. [Test] Positive : {}, Negative: {},  Ratio: {:.2f}'.format(\n",
    "        len(y_train[y_train==1]), len(y_train[y_train==0]), len(y_train[y_train==1])/len(y_train),\n",
    "        len(y_test[y_test==1]), len(y_test[y_test==0]), len(y_test[y_test==1])/len(y_test)\n",
    "    ))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Top10_Features, y, test_size=1/3, random_state=RANDOM_STATE)\n",
    "\n",
    "# tuning max_depth\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_max_depth = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_max_depth = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"red\", label = \"depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_leaf\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_leaf = 0\n",
    "for i in range(10,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_leaf = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(10,20), res, color = \"blue\", label = \"min leaf\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_split\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_split = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, min_samples_split=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_split = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"blue\", label = \"best_min_samples_split\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Best depth = {best_max_depth}, best min_samples_leaf = {best_min_samples_leaf}, best min_sample_split = {best_min_samples_split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682badb0-105b-4745-b9ff-22a502dcc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_RF = []\n",
    "score_PDT = []\n",
    "score_DT = []\n",
    "score_DS = []\n",
    "\n",
    "for i in range(100):\n",
    "    # step 1: train test split of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_Top10_Features, y, test_size=1/3, random_state=RANDOM_STATE + i)\n",
    "\n",
    "    # step 2: Random Forest\n",
    "    score_RF.append(RandomForestClassifier(n_estimators=20, random_state = RANDOM_STATE, n_jobs=-1).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 3: Pruned Decision Tree, pruning is on validation set\n",
    "    \n",
    "    score_PDT.append(DecisionTreeClassifier(max_depth = best_max_depth, min_samples_leaf=best_min_samples_leaf).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 4: Unpruned Decision Tree\n",
    "    score_DT.append(DecisionTreeClassifier(random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 5: Dicision stamp\n",
    "    score_DS.append(DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "# step 6: Compare the models\n",
    "df = pd.DataFrame()\n",
    "df[\"Random Forest\"] = score_RF\n",
    "df[\"Pruned Decision Tree\"] = score_PDT\n",
    "df[\"Unpruned Decision Tree\"] = score_DT\n",
    "df[\"Decision Stamp\"] = score_DS\n",
    "print(df)\n",
    "result = autorank(df,alpha=0.05, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "plot_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46aa6a1-f564-46e5-b6d5-efeb8f3c7099",
   "metadata": {},
   "source": [
    "Task 4\n",
    "----\n",
    "Apply 20% normal additive noise to the features. And then test the models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98699b-78a7-49cb-a14a-0bc1ece395a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: add noise to the data\n",
    "noise = np.random.normal(0, 0.2, np.shape(X_Top10_Features))\n",
    "X_noise_add = X_Top10_Features + np.multiply(noise, np.average(X_Top10_Features, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a19c3c-22ea-4316-931c-df8e107cb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As for pruning, in order to find out the best hyper parameters for pruning, we can use cross validation to test the score for each parameter\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_noise_add, y, test_size=1/3, random_state=RANDOM_STATE)\n",
    "\n",
    "# tuning max_depth\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_max_depth = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_max_depth = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"red\", label = \"depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_leaf\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_leaf = 0\n",
    "for i in range(10,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_leaf = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(10,20), res, color = \"blue\", label = \"min leaf\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_split\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_split = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, min_samples_split=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_split = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"blue\", label = \"best_min_samples_split\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Best depth = {best_max_depth}, best min_samples_leaf = {best_min_samples_leaf}, best min_sample_split = {best_min_samples_split}\")\n",
    "\n",
    "\n",
    "# train and test the models\n",
    "score_RF = []\n",
    "score_PDT = []\n",
    "score_DT = []\n",
    "score_DS = []\n",
    "\n",
    "for i in range(100):\n",
    "    # step 1: train test split of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_noise_add, y, test_size=1/3, random_state=RANDOM_STATE + i)\n",
    "\n",
    "    # step 2: Random Forest\n",
    "    score_RF.append(RandomForestClassifier(n_estimators=20, random_state = RANDOM_STATE, n_jobs=-1).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 3: Pruned Decision Tree, pruning is on validation set\n",
    "    \n",
    "    score_PDT.append(DecisionTreeClassifier(max_depth = best_max_depth, min_samples_leaf=best_min_samples_leaf).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 4: Unpruned Decision Tree\n",
    "    score_DT.append(DecisionTreeClassifier(random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 5: Dicision stamp\n",
    "    score_DS.append(DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "# step 6: Compare the models\n",
    "df = pd.DataFrame()\n",
    "df[\"Random Forest\"] = score_RF\n",
    "df[\"Pruned Decision Tree\"] = score_PDT\n",
    "df[\"Unpruned Decision Tree\"] = score_DT\n",
    "df[\"Decision Stamp\"] = score_DS\n",
    "print(df)\n",
    "result = autorank(df,alpha=0.05, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "plot_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f9a69-6872-4c7d-8705-6c2b60ee4148",
   "metadata": {},
   "source": [
    "Task 5\n",
    "----\n",
    "Add multiplicative normal noise to the features data. And perform the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a470c5-3cdc-462a-a497-462fb7805409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: miltply data matrix with noise matrix\n",
    "noise = np.random.normal(1, 0.2, np.shape(X_Top10_Features))\n",
    "X_noise_multiply = np.multiply(X_Top10_Features, noise)\n",
    "X_noise_multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6453c3-f05d-4a6d-8969-ab953ef09a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As for pruning, in order to find out the best hyper parameters for pruning, we can use cross validation to test the score for each parameter\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_noise_multiply, y, test_size=1/3, random_state=RANDOM_STATE)\n",
    "\n",
    "# tuning max_depth\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_max_depth = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_max_depth = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"red\", label = \"depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_leaf\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_leaf = 0\n",
    "for i in range(10,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_leaf = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(10,20), res, color = \"blue\", label = \"min leaf\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_split\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_split = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, min_samples_split=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_split = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"blue\", label = \"best_min_samples_split\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Best depth = {best_max_depth}, best min_samples_leaf = {best_min_samples_leaf}, best min_sample_split = {best_min_samples_split}\")\n",
    "\n",
    "\n",
    "# This is for training and testing the models\n",
    "score_RF = []\n",
    "score_PDT = []\n",
    "score_DT = []\n",
    "score_DS = []\n",
    "\n",
    "for i in range(100):\n",
    "    # step 1: train test split of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_noise_multiply, y, test_size=1/3, random_state=RANDOM_STATE + i)\n",
    "\n",
    "    # step 2: Random Forest\n",
    "    score_RF.append(RandomForestClassifier(n_estimators=20, random_state = RANDOM_STATE, n_jobs=-1).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 3: Pruned Decision Tree, pruning is on validation set\n",
    "    \n",
    "    score_PDT.append(DecisionTreeClassifier(max_depth = best_max_depth, min_samples_leaf=best_min_samples_leaf).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 4: Unpruned Decision Tree\n",
    "    score_DT.append(DecisionTreeClassifier(random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 5: Dicision stamp\n",
    "    score_DS.append(DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "# step 6: Compare the models\n",
    "df = pd.DataFrame()\n",
    "df[\"Random Forest\"] = score_RF\n",
    "df[\"Pruned Decision Tree\"] = score_PDT\n",
    "df[\"Unpruned Decision Tree\"] = score_DT\n",
    "df[\"Decision Stamp\"] = score_DS\n",
    "print(df)\n",
    "result = autorank(df,alpha=0.05, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "plot_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166ef15-47e2-4d32-85d9-d53cbfec58b4",
   "metadata": {},
   "source": [
    "Task 6\n",
    "----\n",
    "Apply 5% class noise to the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0110ca2-3f3e-48b9-9bc3-1b466fccf60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: flip 5% of the labels randomly\n",
    "import random\n",
    "y_np = y.to_numpy()\n",
    "for i in range(len(y_np)):\n",
    "    getChoice = random.random()\n",
    "    if getChoice < 0.05:\n",
    "#         print(f\"Flip index: {i}\")\n",
    "        y_np[i] = 1 - y_np[i]      \n",
    "\n",
    "# As for pruning, in order to find out the best hyper parameters for pruning, we can use cross validation to test the score for each parameter\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Top10_Features, y_np, test_size=1/3, random_state=RANDOM_STATE)\n",
    "\n",
    "# tuning max_depth\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_max_depth = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_max_depth = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"red\", label = \"depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_leaf\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_leaf = 0\n",
    "for i in range(10,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_leaf = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(10,20), res, color = \"blue\", label = \"min leaf\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_split\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_split = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, min_samples_split=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_split = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"blue\", label = \"best_min_samples_split\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Best depth = {best_max_depth}, best min_samples_leaf = {best_min_samples_leaf}, best min_sample_split = {best_min_samples_split}\")\n",
    "\n",
    "\n",
    "# This is for training and testing the models\n",
    "score_RF = []\n",
    "score_PDT = []\n",
    "score_DT = []\n",
    "score_DS = []\n",
    "\n",
    "for i in range(100):\n",
    "    # step 1: train test split of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_Top10_Features, y_np, test_size=1/3, random_state=RANDOM_STATE + i)\n",
    "\n",
    "    # step 2: Random Forest\n",
    "    score_RF.append(RandomForestClassifier(n_estimators=20, random_state = RANDOM_STATE, n_jobs=-1).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 3: Pruned Decision Tree, pruning is on validation set\n",
    "    \n",
    "    score_PDT.append(DecisionTreeClassifier(max_depth = best_max_depth, min_samples_leaf=best_min_samples_leaf).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 4: Unpruned Decision Tree\n",
    "    score_DT.append(DecisionTreeClassifier(random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 5: Dicision stamp\n",
    "    score_DS.append(DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "# step 6: Compare the models\n",
    "df = pd.DataFrame()\n",
    "df[\"Random Forest\"] = score_RF\n",
    "df[\"Pruned Decision Tree\"] = score_PDT\n",
    "df[\"Unpruned Decision Tree\"] = score_DT\n",
    "df[\"Decision Stamp\"] = score_DS\n",
    "print(df)\n",
    "result = autorank(df,alpha=0.05, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "plot_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e093a-9570-4c38-99d5-fe45eb27f1d5",
   "metadata": {},
   "source": [
    "Task 7\n",
    "----\n",
    "Split the data first and then multiply 20% noise to the training set. Then compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb63e9-a728-47f1-a297-3a9087914f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split the data and add noise to the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Top10_Features, y, test_size=1/3, random_state=RANDOM_STATE)\n",
    "\n",
    "noise = np.random.normal(1, 0.2, np.shape(X_train))\n",
    "X_train_noise = np.multiply(X_train, noise)\n",
    "\n",
    "\n",
    "# As for pruning, in order to find out the best hyper parameters for pruning, we can use cross validation to test the score for each parameter\n",
    "\n",
    "# tuning max_depth\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_max_depth = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train_noise, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_max_depth = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"red\", label = \"depth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_leaf\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_leaf = 0\n",
    "for i in range(10,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train_noise, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_leaf = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(10,20), res, color = \"blue\", label = \"min leaf\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# tuning min_samples_split\n",
    "res = []\n",
    "bestScore = 0\n",
    "best_min_samples_split = 0\n",
    "for i in range(1,20):\n",
    "    model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, min_samples_split=i, random_state = RANDOM_STATE)\n",
    "    score = cross_val_score(model, X_train_noise, y_train, cv = 10).mean()\n",
    "    if score > bestScore:\n",
    "        best_min_samples_split = i\n",
    "        bestScore = score\n",
    "    res.append(score)\n",
    "plt.plot(range(1,20), res, color = \"blue\", label = \"best_min_samples_split\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Best depth = {best_max_depth}, best min_samples_leaf = {best_min_samples_leaf}, best min_sample_split = {best_min_samples_split}\")\n",
    "\n",
    "\n",
    "# This is for training and testing the models\n",
    "score_RF = []\n",
    "score_PDT = []\n",
    "score_DT = []\n",
    "score_DS = []\n",
    "\n",
    "for i in range(100):\n",
    "    # step 1: train test split of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_Top10_Features, y_np, test_size=1/3, random_state=RANDOM_STATE + i)\n",
    "    \n",
    "    noise = np.random.normal(1, 0.2, np.shape(X_train))\n",
    "    X_train_noise = np.multiply(X_train, noise)\n",
    "\n",
    "    # step 2: Random Forest\n",
    "    score_RF.append(RandomForestClassifier(n_estimators=20, random_state = RANDOM_STATE, n_jobs=-1).fit(X_train_noise, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 3: Pruned Decision Tree, pruning is on validation set\n",
    "    \n",
    "    score_PDT.append(DecisionTreeClassifier(max_depth = best_max_depth, min_samples_leaf=best_min_samples_leaf).fit(X_train_noise, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 4: Unpruned Decision Tree\n",
    "    score_DT.append(DecisionTreeClassifier(random_state=RANDOM_STATE).fit(X_train_noise, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 5: Dicision stamp\n",
    "    score_DS.append(DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE).fit(X_train_noise, y_train).score(X_test, y_test))\n",
    "\n",
    "# step 6: Compare the models\n",
    "df = pd.DataFrame()\n",
    "df[\"Random Forest\"] = score_RF\n",
    "df[\"Pruned Decision Tree\"] = score_PDT\n",
    "df[\"Unpruned Decision Tree\"] = score_DT\n",
    "df[\"Decision Stamp\"] = score_DS\n",
    "print(df)\n",
    "result = autorank(df,alpha=0.05, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "plot_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893a95c-0252-4dbc-9d8a-e0f7c9d324b6",
   "metadata": {},
   "source": [
    "Bonus task\n",
    "----\n",
    "Identify how higher the noise is that will break the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a412e-24ee-47f7-b9e9-e928babe3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findParamsPrune(X, y):\n",
    "#     best_max_depth = 0\n",
    "#     bestScore = 0\n",
    "#     for i in range(1,20):\n",
    "#         model = DecisionTreeClassifier(max_depth=i, random_state = RANDOM_STATE)\n",
    "#         score = cross_val_score(model, X, y, cv = 10).mean()\n",
    "#         if score > bestScore:\n",
    "#             best_min_samples_leaf = i\n",
    "#             bestScore = score\n",
    "#     best_min_samples_leaf = 0\n",
    "#     bestScore = 0\n",
    "#     for i in range(10,20):\n",
    "#         model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=i, random_state = RANDOM_STATE)\n",
    "#         score = cross_val_score(model, X, y, cv = 10).mean()\n",
    "#         if score > bestScore:\n",
    "#             best_min_samples_leaf = i\n",
    "#             bestScore = score        \n",
    "#     bestScore = 0\n",
    "#     best_min_samples_split = 0\n",
    "#     for i in range(1,20):\n",
    "#         model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, min_samples_split=i, random_state = RANDOM_STATE)\n",
    "#         score = cross_val_score(model, X, y, cv = 10).mean()\n",
    "#         if score > bestScore:\n",
    "#             best_min_samples_split = i\n",
    "#             bestScore = score\n",
    "#     return (best_max_depth, best_min_samples_leaf, best_min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132895c9-130b-429b-8bb2-844fa8a37f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise added to features, at 20% all works fine, so we add percentage of noise\n",
    "res1 = []\n",
    "res2 = []\n",
    "res3 = []\n",
    "res4 = []\n",
    "interval = (0, 100)\n",
    "for i in range(*interval):\n",
    "    noise = np.random.normal(0, i/100.0, np.shape(X_Top10_Features))\n",
    "    X_noise_add = X_Top10_Features + np.multiply(noise, np.average(X_Top10_Features, axis=0))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_noise_add, y, test_size=1/3, random_state=RANDOM_STATE)\n",
    "\n",
    "    # step 2: Random Forest\n",
    "    res1.append(RandomForestClassifier(n_estimators=20, random_state = RANDOM_STATE, n_jobs=-1).fit(X_train, y_train).score(X_test, y_test))\n",
    "    \n",
    "    # step 3: Pruned Decision Tree, pruning is on validation set\n",
    "\n",
    "    res2.append(DecisionTreeClassifier(max_depth = 5, min_samples_leaf=10).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 4: Unpruned Decision Tree\n",
    "    res3.append(DecisionTreeClassifier(random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "    # step 5: Dicision stamp\n",
    "    res4.append(DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE).fit(X_train, y_train).score(X_test, y_test))\n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(range(*interval), res1, color = \"red\", label = \"Random Forest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(*interval), res2, color = \"red\", label = \"Pruned Decision tree\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(*interval), res3, color = \"red\", label = \"Unpruned Decision tree\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(*interval), res4, color = \"red\", label = \"Decision Stamp\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622da7e-27f2-47e7-9b2b-bc025edd2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise multiplied to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dca95-a1fc-4c1f-9c7a-f9d272fc0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise flipping the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe30fb7-ea47-4fc8-a355-ff935a933319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise added to the training features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
